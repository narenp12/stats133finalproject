---
title: "stats 133 project - data cleaning/processing"
author: "Caleb Williams"
date: '`r Sys.Date()`'
output: pdf_document
---

```{r, message = FALSE, warning = FALSE}

library(dplyr)
library(tidyr)
library(tidytext)
library(tibble)
library(tm)
library(syuzhet)
library(pdftools)
library(quanteda)
library(quanteda.corpora)
library(quanteda.textstats)
library(stringr)
library(ggplot2)
library(readtext)
library(reshape2)
library(wordcloud)
library(wordcloud2)
library(SnowballC)
library(purrr)
library(RColorBrewer)
library(ggfortify)
library(widyr)
library(gender)
library(ggraph)             
library(igraph)            
library(stringdist)

```


```{r}
set.seed(133)

movie_text <- read.csv('/Users/calebwilliams/Downloads/stats133finalproject/title_year_text.csv')

head(movie_text, 10)

```

# Text Cleaning
```{r}

sets <- data(package = "genderdata")$results[,"Item"]
data(list = sets, package = "genderdata")
stopwords_names <- unique(genderdata::kantrowitz$name)


removeWordsChunked <- function(txt, words, chunk_size = 500) {
  if (length(words) == 0) return(txt)  # Handle empty word list case
  
  words_split <- split(words, ceiling(seq_along(words) / chunk_size))
  
  for (word_group in words_split) {
    pattern <- paste0("\\b(", paste(word_group, collapse = "|"), ")\\b")
    txt <- gsub(pattern, "", txt, perl = TRUE, ignore.case = TRUE)
  }
  
  return(txt)
}
clean_text <- function(text) {
  
  # remove names
  text <- removeWordsChunked(text, stopwords_names)

  text <- removePunctuation(text) 
  text <- gsub("[/@|\\\\\"]", " ", text)


  
  text <- tolower(text)
  text <- removeNumbers(text)
  text <- removeWords(text, stopwords("en"))
  text <- stripWhitespace(text)
  text <- stemDocument(text)

  
  # specific transformations
  text <- gsub("mmddyyyy", " ", text)
  text <- removeWords(text, c("tin", "cup", "juno"))
  text <- removeWords(text, c("gestur", "offscreen", "cut", "continu", "page", "walk", "camera", "scriptscom")) 
  text <- gsub("â€¢", "", text)


  
  

  return(text)
}

# apply cleaning function
movie_text$text <- sapply(movie_text$text, clean_text)

```

```{r}
head(movie_text)
```

```{r}

cleaned_text_decade <- movie_text %>%
  mutate(decade = paste0(floor(year / 10) * 10, "s"))

head(cleaned_text_decade) # cleaned corpus with decade
```

# Export movie_text_decade to csv
```{r}
write.csv(cleaned_text_decade, 'cleaned_text_decade.csv')
```




# Tokenization
```{r}
text_tokens <- movie_text %>%
  select(title, text) %>%
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word")

```

# Convert to tidy and tf_idf format 
```{r}
# tidy format
tidy_text <- text_tokens %>%
  count(title, word) %>% 
  arrange(desc(n))  

head(tidy_text, 10)
```

```{r}
# tf_idf format
text_tf_idf <- tidy_text %>%                       
  bind_tf_idf(word, title, n) %>%      
  arrange(desc(tf_idf))  

text_tf_idf

```


```{r}
# just year and title 
year_title <- movie_text[,c('year','title')]
year_title


```

# Add year to tf_idf
```{r}
tf_idf_year <- inner_join(year_title, text_tf_idf, by = "title")

head(tf_idf_year)

```

# Add decade to tf_idf_year
```{r}

# Add decade column
tf_idf_decade <- tf_idf_year %>%
  mutate(decade = paste0(floor(year / 10) * 10, "s"))


head(tf_idf_decade)

```



# Export tf_idf format to csv
```{r}
write.csv(tf_idf_decade, 'movie_texts_tf_idf.csv')
```




# Tidy to Dtm
```{r}
dtm <- tidy_text %>% 
  cast_dtm(title, word, n)

#dtm %>% inspect()

# remove sparse terms 
dtm <- removeSparseTerms(dtm, 0.96)


dtm %>% inspect()

```

# Lexical Diversity
```{r}

# dtm to dfm (dtm -> tidy -> dfm)

tidy_dtm <- tidy(dtm)

dfm <- tidy_dtm %>%
  cast_dfm(document, term, count)  

head(dfm)

```

```{r}
# lexical diversity
lexdiv <- textstat_lexdiv(dfm)
lexdiv

lexdiv <- lexdiv %>% rename(title = document)

```

```{r}
#  unique title-year-decade pairs
unique_years <- tf_idf_decade %>%
  select(title, year, decade) %>%
  distinct()

lexdiv_decade <- lexdiv %>%
  left_join(unique_years, by = "title")

lexdiv_decade

```
```{r}
lexdiv_summary <- lexdiv_decade %>%
  group_by(decade) %>%
  summarize(mean_TTR = mean(TTR)) # get mean TTR per decade 

ggplot(lexdiv_summary, aes(x = decade, y = mean_TTR, group = 1)) +
  geom_line(color = "mediumpurple1", size = 1) +  
  geom_point(color = "violet", size = 2) +  
  labs(title = "Mean TTR by Decade",
       x = "Decade",
       y = "Mean TTR") +
  theme_minimal()

```

# ANOVA to see if difference in TTR between decades is significant
```{r}
model <- lm(TTR ~ decade, data = lexdiv_decade)
summary(model)

# check normality assumptions
par(mfrow=c(2,2))
autoplot(model)

# they look good!

# p-value: 0.2078 > 0.05
# differences are NOT significant!! 
```




# Word Frequency Per Decade
```{r}
top_words_per_decade <- tf_idf_decade %>%
  group_by(decade, word) %>%
  summarize(total_n = sum(n), .groups = "drop") %>%  
  arrange(decade, desc(total_n)) %>%
  group_by(decade) %>%
  slice_head(n = 15) # select top n words


top_words_per_decade

```

```{r}
ggplot(top_words_per_decade, aes(x = reorder_within(word, total_n, decade), y = total_n, fill = decade)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~decade, scales = "free") +  
  coord_flip() +
  labs(title = "Top 15 Most Frequent Words Per Decade",
       x = "Word",
       y = "Frequency") +
  theme_minimal() + 
  scale_fill_manual(values = c(
    "1980s" = "lightcoral",
    "1990s" = "pink1",
    "2000s" = "thistle2",
    "2010s" = "mediumpurple1"
  )) +
  scale_x_reordered() 

```
# Word clouds
```{r}

top_words_per_decade_2 <- tf_idf_decade %>%
  group_by(decade, word) %>%
  summarize(total_n = sum(n), .groups = "drop") %>%  
  arrange(decade, desc(total_n)) %>%
  group_by(decade) %>%
  slice_head(n = 50) # select top 50 words




count_80s <- top_words_per_decade_2 %>% filter(decade == '1980s')
count_90s <- top_words_per_decade_2 %>% filter(decade == '1990s')
count_00s <- top_words_per_decade_2 %>% filter(decade == '2000s')
count_10s <- top_words_per_decade_2 %>% filter(decade == '2010s')
```

```{r}

# 1980s
wordcloud(
  words = count_80s$word, 
  freq = count_80s$total_n,   
  max.words = 30,           
  colors = brewer.pal(8, "Set1"),
  main = "1980s Word Cloud"
)

# 1990s
wordcloud(
  words = count_90s$word, 
  freq = count_90s$total_n,   
  max.words = 30,           
  colors = brewer.pal(8, "Set1"),
  main = "1990s Word Cloud"
)

# 2000s
wordcloud(
  words = count_00s$word, 
  freq = count_00s$total_n,   
  max.words = 30,           
  colors = brewer.pal(8, "Set1"),
  main = "1990s Word Cloud"
)


# 2010s
wordcloud(
  words = count_10s$word, 
  freq = count_10s$total_n,   
  max.words = 30,           
  colors = brewer.pal(8, "Set1"),
  main = "1990s Word Cloud"
)




```



# Bigrams + Correlation Plots
```{r}
movie_bigrams <- cleaned_text_decade %>% unnest_tokens(bigram, text, token = "ngrams", n = 2)

head(movie_bigrams,10)
```

```{r}

movie_bigrams_separated <- movie_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  pivot_longer(cols = c(word1, word2), names_to = "position", values_to = "word") %>%
  select(-position)  

word_counts <- movie_bigrams_separated %>%
  count(word, sort = TRUE)

word_cors <- movie_bigrams_separated %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, title, sort = TRUE) %>% 
  drop_na()

sample_n(word_cors,10)

```


```{r}

plot_word_correlation <- function(movie_bigrams, n_top = 125, corr_threshold = 0.15) {
  # get word correlations
  word_cors <- movie_bigrams %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    count(word1, word2, sort = TRUE) %>%
    filter(n >= 10) %>%  # Filter for words appearing at least 10 times
    pairwise_cor(word1, word2, sort = TRUE) 
  
  # top correlated word pairs
  top_words <- word_cors %>%
    filter(correlation > corr_threshold) %>%  # Apply correlation threshold
    arrange(desc(correlation)) %>%
    slice_head(n = n_top) 
  
  word_graph <- graph_from_data_frame(top_words)
  
  # plot with ggraph
  ggraph(word_graph, layout = "fr") +
    geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
}

```



```{r}
bigrams_80s <- movie_bigrams %>% filter(decade == "1980s")
bigrams_90s <- movie_bigrams %>% filter(decade == "1990s")
bigrams_00s <- movie_bigrams %>% filter(decade == "2000s")
bigrams_10s <- movie_bigrams %>% filter(decade == "2010s")
```

```{r}
plot_word_correlation(bigrams_80s)
```
```{r}
plot_word_correlation(bigrams_90s)
```
```{r}
plot_word_correlation(bigrams_00s)
```
```{r}
plot_word_correlation(bigrams_10s)
```


# Trigrams + Correlation Plots
```{r}
movie_trigrams <- cleaned_text_decade %>% unnest_tokens(trigram, text, token = "ngrams", n = 3)

head(movie_trigrams,10)
```

```{r}

plot_word_correlation_tri <- function(movie_trigrams, n_top = 125, corr_threshold = 0.15) {
  library(tidyverse)
  library(widyr)
  library(igraph)
  library(ggraph)

  if (!"title" %in% colnames(movie_trigrams)) {
    stop("Error: The input data must contain a 'title' column.")
  }

  word_cors <- movie_trigrams %>%
    separate(trigram, c("word1", "word2", "word3"), sep = " ", remove = FALSE) %>%
    pivot_longer(cols = c(word1, word2, word3), names_to = "position", values_to = "word") %>%
    select(title, word) %>%  
    group_by(word) %>%
    filter(n() >= 20) %>%
    pairwise_cor(word, title, sort = TRUE) %>%
    drop_na()

  top_words <- word_cors %>%
    filter(correlation > corr_threshold) %>%  # Apply correlation threshold
    arrange(desc(correlation)) %>%
    slice_head(n = n_top)

  word_graph <- graph_from_data_frame(top_words)

  ggraph(word_graph, layout = "fr") +
    geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), repel = TRUE) +
    theme_void()
}

```

```{r}
trigrams_80s <- movie_trigrams %>% filter(decade == "1980s")
trigrams_90s <- movie_trigrams %>% filter(decade == "1990s")
trigrams_00s <- movie_trigrams %>% filter(decade == "2000s")
trigrams_10s <- movie_trigrams %>% filter(decade == "2010s")
```

```{r}
plot_word_correlation_tri(trigrams_80s)
```
```{r}
plot_word_correlation_tri(trigrams_90s)
```
```{r}
plot_word_correlation_tri(trigrams_00s)
```
```{r}
plot_word_correlation_tri(trigrams_10s)
```

# Word Co-Occurences
```{r}

word_pairs <- tf_idf_decade %>% 
  pairwise_count(word, title, sort = TRUE, upper = FALSE)


word_pairs_filtered <- word_pairs %>%
  arrange(desc(n)) %>%
  slice_head(n = 100)

head(word_pairs_filtered)
```

# top word pairs, not by decade
```{r}
word_pairs_network <- word_pairs_filtered %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4", alpha=0.35) +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, "lines"), max.overlaps = 100) +
  theme_void()

word_pairs_network
```

# per decade
```{r}
pairs_80s <- tf_idf_decade %>%
  filter(decade == "1980s") %>% 
  select(title, word, n) %>% 
  pairwise_count(word, title, sort = TRUE, upper = FALSE) %>%  
  arrange(desc(n)) %>% 
  slice_head(n = 100)



head(pairs_80s)
```
```{r}
pairs_80s %>%
  filter(n >= 5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()


```



```{r}

pairs_90s <- tf_idf_decade %>%
  filter(decade == "1980s") %>% 
  select(title, word, n) %>% 
  pairwise_count(word, title, sort = TRUE, upper = FALSE) %>%  
  arrange(desc(n)) %>% 
  slice_head(n = 100)



head(pairs_90s)

pairs_90s %>%
  filter(n >= 5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()

```


```{r}



pairs_00s <- tf_idf_decade %>%
  filter(decade == "1980s") %>% 
  select(title, word, n) %>% 
  pairwise_count(word, title, sort = TRUE, upper = FALSE) %>%  
  arrange(desc(n)) %>% 
  slice_head(n = 100)



head(pairs_00s)

pairs_00s %>%
  filter(n >= 5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()


```

```{r}



pairs_10s <- tf_idf_decade %>%
  filter(decade == "1980s") %>% 
  select(title, word, n) %>% 
  pairwise_count(word, title, sort = TRUE, upper = FALSE) %>%  
  arrange(desc(n)) %>% 
  slice_head(n = 100)



head(pairs_10s)

pairs_10s %>%
  filter(n >= 5) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()


```








# network with all decades 
```{r}

word_pairs_by_decade <- tf_idf_decade %>% 
  group_by(decade) %>% 
  pairwise_count(word, title, sort = TRUE, upper = FALSE) %>% 
  ungroup() 

head(word_pairs_by_decade)

```

```{r}


# keeping only the top 5 most frequent word pairs per decade
word_pairs_top <- word_pairs_by_decade %>%
  group_by(decade) %>%
  slice_max(n, n = 5) %>%
  ungroup()
```


```{r}
edges <- word_pairs_top %>%
  rename(from = decade, to = item1) %>%
  select(from, to, n)
```

```{r}
word_graph <- graph_from_data_frame(edges)
```

```{r}
ggraph(word_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4", alpha = 0.9) +
  geom_node_point(size = 2, color = "lightblue") +
  geom_node_text(aes(label = name), repel = TRUE, fontface = "bold", point.padding = unit(0.2, "lines"), max.overlaps = 100) +
  theme_void()

```



