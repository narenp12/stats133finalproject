---
title: "stats 133 project - data cleaning/processing"
author: "Caleb Williams"
date: '`r Sys.Date()`'
output: pdf_document
---

```{r, message = FALSE, warning = FALSE}

library(dplyr)
library(tidyr)
library(tidytext)
library(tibble)
library(tm)
library(syuzhet)
library(pdftools)
library(quanteda)
library(quanteda.corpora)
library(quanteda.textstats)
library(stringr)
library(ggplot2)
library(readtext)
library(reshape2)
library(wordcloud)
library(wordcloud2)
library(SnowballC)
library(purrr)
library(RColorBrewer)
library(ggfortify)

```


```{r}
set.seed(133)

movie_text <- read.csv('/Users/calebwilliams/Downloads/stats133finalproject/title_year_text.csv')

head(movie_text, 10)
```

# Text Cleaning
```{r}

clean_text <- function(text) {
  text <- tolower(text)
  text <- removeNumbers(text)
  text <- removePunctuation(text) 
  text <- removeWords(text, stopwords("en"))
  text <- stripWhitespace(text)
  text <- stemDocument(text)
  
  # specific transformations
  text <- gsub("mmddyyyy", " ", text)

  return(text)
}

# apply cleaning function
movie_text$text <- sapply(movie_text$text, clean_text)

```


# Tokenization
```{r}
text_tokens <- movie_text %>%
  select(title, text) %>%              
  unnest_tokens(word, text)

```

# Convert to tidy and tf_idf format 
```{r}
# tidy format
tidy_text <- text_tokens %>%
  count(title, word) %>% 
  arrange(desc(n))  

head(tidy_text, 10)
```

```{r}
# tf_idf format
text_tf_idf <- tidy_text %>%                       
  bind_tf_idf(word, title, n) %>%      
  arrange(desc(tf_idf))  

head(text_tf_idf, 10)

```


```{r}
# just year and title 
year_title <- movie_text[,c('year','title')]
year_title


```

# Add year to tf_idf
```{r}
tf_idf_year <- inner_join(year_title, text_tf_idf, by = "title")

head(tf_idf_year)

```

# Add decade to tf_idf_year
```{r}

# Add decade column
tf_idf_decade <- tf_idf_year %>%
  mutate(decade = paste0(floor(year / 10) * 10, "s"))


head(tf_idf_decade)


```



# Export tf_idf format to csv
```{r}
write.csv(tf_idf_decade, 'movie_texts_tf_idf.csv')
```




# Tidy to Dtm
```{r}
dtm <- tidy_text %>% 
  cast_dtm(title, word, n)

#dtm %>% inspect()

# remove sparse terms 
dtm <- removeSparseTerms(dtm, 0.96)


dtm %>% inspect()

```

# Lexical Diversity
```{r}

# dtm to dfm (dtm -> tidy -> dfm)

tidy_dtm <- tidy(dtm)

dfm <- tidy_dtm %>%
  cast_dfm(document, term, count)  

head(dfm)

```

```{r}
# lexical diversity
lexdiv <- textstat_lexdiv(dfm)
lexdiv

lexdiv <- lexdiv %>% rename(title = document)

```

```{r}
#  unique title-year-decade pairs
unique_years <- tf_idf_decade %>%
  select(title, year, decade) %>%
  distinct()

lexdiv_decade <- lexdiv %>%
  left_join(unique_years, by = "title")

lexdiv_decade

```
```{r}
lexdiv_summary <- lexdiv_decade %>%
  group_by(decade) %>%
  summarize(mean_TTR = mean(TTR)) # get mean TTR per decade 

ggplot(lexdiv_summary, aes(x = decade, y = mean_TTR, group = 1)) +
  geom_line(color = "mediumpurple1", size = 1) +  
  geom_point(color = "violet", size = 2) +  
  labs(title = "Mean TTR by Decade",
       x = "Decade",
       y = "Mean TTR") +
  theme_minimal()

```

# ANOVA to see if difference in TTR between decades is significant
```{r}
model <- lm(TTR ~ decade, data = lexdiv_decade)
summary(model)

# check normality assumptions
par(mfrow=c(2,2))
autoplot(model)

# they look good!

# p-value: 0.267
# differences are NOT significant!! 
```




# Word Frequency Per Decade
```{r}
top_words_per_decade <- tf_idf_decade %>%
  group_by(decade, word) %>%
  summarize(total_n = sum(n), .groups = "drop") %>%  
  arrange(decade, desc(total_n)) %>%
  group_by(decade) %>%
  slice_head(n = 10) # select top 10 words


top_words_per_decade

```

```{r}
ggplot(top_words_per_decade, aes(x = reorder_within(word, total_n, decade), y = total_n, fill = decade)) +
  geom_col(show.legend = FALSE) + 
  facet_wrap(~decade, scales = "free") +  
  coord_flip() +
  labs(title = "Top 10 Most Frequent Words Per Decade",
       x = "Word",
       y = "Frequency") +
  theme_minimal() + 
  scale_fill_manual(values = c(
    "1980s" = "lightcoral",
    "1990s" = "pink1",
    "2000s" = "thistle2",
    "2010s" = "mediumpurple1"
  )) +
  scale_x_reordered() 

```



```{r}
# correlation analysis of documents within each decade -- all rom coms of the 90s, etc. 



```


# N-grams
```{r}



```

